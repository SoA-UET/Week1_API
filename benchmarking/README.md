# API Benchmarking Suite

**GENERATED BY CLAUDE**

A comprehensive benchmarking tool for comparing performance across different API architectures: **REST**, **GraphQL**, **gRPC**, and **SOAP**.

- [API Benchmarking Suite](#api-benchmarking-suite)
  - [📋 Overview](#-overview)
    - [API Types Tested](#api-types-tested)
    - [Operations Benchmarked](#operations-benchmarked)
  - [🚀 Quick Start](#-quick-start)
    - [Prerequisites](#prerequisites)
    - [Installation](#installation)
    - [Running Benchmarks](#running-benchmarks)
  - [📊 Understanding Results](#-understanding-results)
    - [Sample Output](#sample-output)
  - [🏗️ Architecture](#️-architecture)
  - [🔧 Manual Server Management](#-manual-server-management)
    - [REST Server](#rest-server)
    - [GraphQL Server](#graphql-server)
    - [gRPC Server](#grpc-server)
    - [SOAP Server](#soap-server)
  - [📈 Benchmark Methodology](#-benchmark-methodology)
    - [Test Data](#test-data)
    - [Benchmark Process](#benchmark-process)
    - [Metrics Collected](#metrics-collected)
  - [📄 Output Files](#-output-files)
  - [🛠️ Customization](#️-customization)
    - [Modifying Test Parameters](#modifying-test-parameters)
    - [Adding New Operations](#adding-new-operations)
    - [Custom Test Data](#custom-test-data)
  - [🚨 Troubleshooting](#-troubleshooting)
    - [Common Issues](#common-issues)
    - [Dependency Issues](#dependency-issues)
  - [📊 Performance Expectations](#-performance-expectations)
  - [🤝 Contributing](#-contributing)
  - [📜 License](#-license)

## 📋 Overview

This benchmarking suite provides a standardized way to measure and compare the performance characteristics of four major API paradigms using a common dataset and identical operations.

### API Types Tested
- **REST API** - FastAPI implementation
- **GraphQL API** - Graphene-based server  
- **gRPC API** - Protocol Buffers with Python
- **SOAP API** - Spyne-based web service

### Operations Benchmarked
- **Get All Users** - List users with pagination
- **Get User By ID** - Retrieve single user
- **Create User** - Add new user
- **Update User** - Modify existing user  
- **Delete User** - Remove user
- **Search Users** - Find users by name/email

## 🚀 Quick Start

### Prerequisites
- Python >=3.8, <=3.11
- pip package manager

### Installation

1. **Clone/Navigate to the benchmarking directory:**
   ```bash
   cd benchmarking/
   ```

2. **Install dependencies:**
   ```bash
   python3.10 -m venv venv
   source ./venv/bin/activate
   pip install -r requirements.txt
   ```

### Running Benchmarks

**Run all API benchmarks (100 requests each):**
```bash
python benchmark.py
```

**Run specific APIs only:**
```bash
python benchmark.py --apis REST GraphQL
```

**Custom number of requests:**
```bash
python benchmark.py --requests 200
```

**Keep servers running between tests:**
```bash
python benchmark.py --keep-servers
```

## 📊 Understanding Results

The benchmark generates several metrics for each operation:

- **Requests/Second**: Throughput measurement
- **Average Response Time**: Mean latency in milliseconds  
- **Min/Max Response Time**: Range of response times
- **Success Rate**: Percentage of successful requests
- **Error Rate**: Percentage of failed requests

Load the JSON into <https://kanaries.net/tools/json-to-chart>
to visualize the numbers.

### Sample Output
```
🏆 REST:
    Requests/sec: 245.32
    Avg Response: 4.08ms
    Success Rate: 100.0%

2. GraphQL:
    Requests/sec: 198.67
    Avg Response: 5.03ms
    Success Rate: 100.0%
```

## 🏗️ Architecture

```
benchmarking/
├── benchmark.py              # Main orchestrator script
├── requirements.txt          # Python dependencies
├── reports/                  # Generated benchmark reports
└── app/
    ├── common/
    │   ├── data/
    │   │   └── users.csv     # Test dataset (500 Vietnamese users)
    │   ├── models.py         # Shared data models
    │   └── utils.py          # Benchmarking utilities
    ├── REST/
    │   ├── server.py         # FastAPI server
    │   └── client.py         # REST client & benchmarks
    ├── GraphQL/
    │   ├── server.py         # Graphene server
    │   └── client.py         # GraphQL client & benchmarks
    ├── gRPC/
    │   ├── proto/
    │   │   └── user_service.proto
    │   ├── server.py         # gRPC server
    │   └── client.py         # gRPC client & benchmarks
    └── SOAP/
        ├── server.py         # Spyne SOAP server
        └── client.py         # SOAP client & benchmarks
```

## 🔧 Manual Server Management

If you need to run servers independently:

### REST Server
```bash
cd app/REST/
python server.py
# Server runs on http://localhost:8000
```

### GraphQL Server
```bash
cd app/GraphQL/
python server.py
# Server runs on http://localhost:8001
# GraphQL endpoint: http://localhost:8001/graphql
```

### gRPC Server
```bash
cd app/gRPC/
python server.py
# Server runs on localhost:50051
```

### SOAP Server
```bash
cd app/SOAP/
python server.py  
# Server runs on http://localhost:8002
# WSDL: http://localhost:8002?wsdl
```

## 📈 Benchmark Methodology

### Test Data
- **500 Vietnamese users** from `users.csv`
- Fields: ID, Name (Vietnamese), Email
- Realistic data for comprehensive testing

### Benchmark Process
1. **Server Startup**: Automatically starts all required servers
2. **Data Loading**: Each server loads the same CSV dataset
3. **Sequential Testing**: Each API type tested in isolation
4. **Operation Testing**: All CRUD operations benchmarked
5. **Report Generation**: Results compiled and compared

### Metrics Collected
- **Response Time Distribution** (min, max, average)
- **Throughput** (requests per second)
- **Error Rates** and success percentages
- **Resource Usage** patterns

## 📄 Output Files

Benchmark results are saved in the `reports/` directory:

- **`benchmark_report_YYYYMMDD_HHMMSS.txt`** - Human-readable comparison report
- **`benchmark_results_YYYYMMDD_HHMMSS.json`** - Raw data for further analysis

## 🛠️ Customization

### Modifying Test Parameters

Edit `benchmark.py` to adjust:
- Number of requests per operation
- Server ports and URLs
- Timeout values
- Test operations

### Adding New Operations

1. Add operation to each API server (`server.py`)
2. Add client method to each client (`client.py`)  
3. Add benchmark method to each benchmark class
4. Update operation list in `run_all_benchmarks()`

### Custom Test Data

Replace `app/common/data/users.csv` with your dataset. Ensure format:
```csv
id,name,email
1,User Name,email@example.com
```

## 🚨 Troubleshooting

### Common Issues

**Port Already in Use:**
```bash
# Kill processes on specific ports
lsof -ti:8000 | xargs kill -9  # REST
lsof -ti:8001 | xargs kill -9  # GraphQL
lsof -ti:8002 | xargs kill -9  # SOAP
lsof -ti:50051 | xargs kill -9 # gRPC
```

**Import Errors:**
```bash
# Ensure all dependencies installed
pip install -r requirements.txt

# For gRPC, protobuf files are auto-generated
cd app/gRPC/
python server.py  # This generates the proto files
```

**Server Not Ready:**
- Increase wait times in `benchmark.py`
- Check server logs for startup errors
- Verify CSV file exists and is readable

### Dependency Issues

If you encounter import errors:

```bash
# Update pip and reinstall
pip install --upgrade pip
pip install -r requirements.txt --force-reinstall
```

For specific libraries:
```bash
# GraphQL dependencies  
pip install graphene fastapi starlette

# gRPC dependencies
pip install grpcio grpcio-tools

# SOAP dependencies
pip install spyne zeep lxml
```

## 📊 Performance Expectations

Based on typical runs:

| API Type | Throughput (req/s) | Avg Latency (ms) | Overhead |
|----------|-------------------|------------------|----------|
| gRPC     | 300-500          | 2-3              | Low      |
| REST     | 200-400          | 3-5              | Low      |
| GraphQL  | 150-300          | 4-7              | Medium   |
| SOAP     | 50-150           | 10-20            | High     |

*Results vary based on hardware, network, and system load*

## 🤝 Contributing

To add support for additional API types:

1. Create new directory under `app/`
2. Implement `server.py` with required operations
3. Implement `client.py` with benchmark methods
4. Add to `server_configs` in `benchmark.py`
5. Update documentation

## 📜 License

This benchmarking suite is provided for educational and comparison purposes.